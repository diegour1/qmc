{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Mixture of patches classification with QMC",
      "provenance": [],
      "collapsed_sections": []
    },
    "environment": {
      "name": "tf2-gpu.2-4.m61",
      "type": "gcloud",
      "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-4:m61"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQ9Ku1h_aYwR"
      },
      "source": [
        "# Mixture of patches classification with QMC\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6tc-zDLY4AUB",
        "outputId": "3a28b670-85cd-412f-ccfd-99f821b39006"
      },
      "source": [
        "# Install qmc if running in Google Colab\n",
        "\n",
        "try:\n",
        "  import google.colab\n",
        "  IN_COLAB = True\n",
        "except:\n",
        "  IN_COLAB = False\n",
        "\n",
        "if IN_COLAB:\n",
        "    !pip install --upgrade  git+https://github.com/fagonzalezo/qmc.git\n",
        "    !pip install -U tensorflow-addons\n",
        "else:\n",
        "    import sys\n",
        "    sys.path.insert(0, \"../\")\n",
        "\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/fagonzalezo/qmc.git\n",
            "  Cloning https://github.com/fagonzalezo/qmc.git to /tmp/pip-req-build-buj_isjb\n",
            "  Running command git clone -q https://github.com/fagonzalezo/qmc.git /tmp/pip-req-build-buj_isjb\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from qmc==0.0.1) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.19.2 in /usr/local/lib/python3.7/dist-packages (from qmc==0.0.1) (1.19.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from qmc==0.0.1) (1.0.1)\n",
            "Requirement already satisfied: tensorflow>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from qmc==0.0.1) (2.7.0)\n",
            "Requirement already satisfied: typeguard in /usr/local/lib/python3.7/dist-packages (from qmc==0.0.1) (2.7.1)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->qmc==0.0.1) (1.15.0)\n",
            "Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->qmc==0.0.1) (0.12.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->qmc==0.0.1) (1.1.0)\n",
            "Requirement already satisfied: keras<2.8,>=2.7.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->qmc==0.0.1) (2.7.0)\n",
            "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->qmc==0.0.1) (12.0.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->qmc==0.0.1) (1.1.2)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->qmc==0.0.1) (3.3.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->qmc==0.0.1) (3.17.3)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->qmc==0.0.1) (0.2.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->qmc==0.0.1) (1.6.3)\n",
            "Requirement already satisfied: tensorflow-estimator<2.8,~=2.7.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->qmc==0.0.1) (2.7.0)\n",
            "Requirement already satisfied: flatbuffers<3.0,>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->qmc==0.0.1) (2.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->qmc==0.0.1) (3.10.0.2)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->qmc==0.0.1) (3.1.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->qmc==0.0.1) (0.22.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->qmc==0.0.1) (1.41.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.32.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->qmc==0.0.1) (0.37.0)\n",
            "Requirement already satisfied: tensorboard~=2.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->qmc==0.0.1) (2.7.0)\n",
            "Requirement already satisfied: gast<0.5.0,>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->qmc==0.0.1) (0.4.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->qmc==0.0.1) (1.13.3)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow>=2.2.0->qmc==0.0.1) (1.5.2)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow>=2.2.0->qmc==0.0.1) (57.4.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow>=2.2.0->qmc==0.0.1) (3.3.4)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow>=2.2.0->qmc==0.0.1) (1.0.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow>=2.2.0->qmc==0.0.1) (2.23.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow>=2.2.0->qmc==0.0.1) (0.4.6)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow>=2.2.0->qmc==0.0.1) (0.6.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow>=2.2.0->qmc==0.0.1) (1.35.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow>=2.2.0->qmc==0.0.1) (1.8.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow>=2.2.0->qmc==0.0.1) (4.7.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow>=2.2.0->qmc==0.0.1) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow>=2.2.0->qmc==0.0.1) (4.2.4)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow>=2.2.0->qmc==0.0.1) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.6->tensorflow>=2.2.0->qmc==0.0.1) (4.8.2)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow>=2.2.0->qmc==0.0.1) (0.4.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow>=2.2.0->qmc==0.0.1) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow>=2.2.0->qmc==0.0.1) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow>=2.2.0->qmc==0.0.1) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow>=2.2.0->qmc==0.0.1) (3.0.4)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow>=2.2.0->qmc==0.0.1) (3.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard~=2.6->tensorflow>=2.2.0->qmc==0.0.1) (3.6.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->qmc==0.0.1) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->qmc==0.0.1) (3.0.0)\n",
            "Building wheels for collected packages: qmc\n",
            "  Building wheel for qmc (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for qmc: filename=qmc-0.0.1-py3-none-any.whl size=12796 sha256=f1332bea5d01d71a2cc87d11d69110ed63b11f817ed2489a0a98058e312a4d00\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-jlaz9vsd/wheels/b2/d2/8d/5870208920445c46dfe694f549251e5f63d7afbee56c01f720\n",
            "Successfully built qmc\n",
            "Installing collected packages: qmc\n",
            "Successfully installed qmc-0.0.1\n",
            "Collecting tensorflow-addons\n",
            "  Downloading tensorflow_addons-0.15.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 12.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow-addons) (2.7.1)\n",
            "Installing collected packages: tensorflow-addons\n",
            "Successfully installed tensorflow-addons-0.15.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FKQ0ZMbdV0M2",
        "outputId": "83d537c0-cfbb-4d0a-ec06-f8e7759c9923"
      },
      "source": [
        "!pwd"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dwx1YXdUaYwT"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v0DXdtVWaYwU"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import tensorflow_addons as tfa\n",
        "import qmc.tf.layers as layers\n",
        "import qmc.tf.models as models"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J6B8j9dBaYwU"
      },
      "source": [
        "## Prepare the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zt6i10fJaYwV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d05b08b-0f80-4a55-bc7d-501f77ebe015"
      },
      "source": [
        "num_classes = 10\n",
        "input_shape = (28, 28, 1)\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
        "\n",
        "x_train = np.expand_dims(x_train, axis=-1)\n",
        "x_test = np.expand_dims(x_test, axis=-1)\n",
        "print(f\"x_train shape: {x_train.shape} - y_train shape: {y_train.shape}\")\n",
        "print(f\"x_test shape: {x_test.shape} - y_test shape: {y_test.shape}\")\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "11501568/11490434 [==============================] - 0s 0us/step\n",
            "x_train shape: (60000, 28, 28, 1) - y_train shape: (60000,)\n",
            "x_test shape: (10000, 28, 28, 1) - y_test shape: (10000,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X0JLTS8WaYwV"
      },
      "source": [
        "## Configure the hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B50WUbDRaYwV"
      },
      "source": [
        "learning_rate = 0.001\n",
        "weight_decay = 0.0001\n",
        "batch_size = 256\n",
        "num_epochs = 10\n",
        "image_size = 28  # We'll resize input images to this size\n",
        "patch_size = 7  # Size of the patches to be extract from the input images\n",
        "num_patches = (image_size // patch_size) ** 2\n",
        "projection_dim = 64\n",
        "input_dim = 64\n",
        "num_rff = 512\n",
        "gamma = 2**-5\n",
        "n_comp = 80\n",
        "random_state = 0\n",
        "\n",
        "mlp_head_units = [256]  # Size of the dense layers of the final classifier\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DD5GJabEaYwV"
      },
      "source": [
        "## Use data augmentation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ESXjO5kiaYwW"
      },
      "source": [
        "data_augmentation = keras.Sequential(\n",
        "    [\n",
        "        keras.layers.Normalization(),\n",
        "        #layers.Resizing(image_size, image_size),\n",
        "        #layers.RandomFlip(\"horizontal\"),\n",
        "        #layers.RandomRotation(factor=0.02),\n",
        "        #layers.RandomZoom(\n",
        "        #    height_factor=0.2, width_factor=0.2\n",
        "        #),\n",
        "    ],\n",
        "    name=\"data_augmentation\",\n",
        ")\n",
        "# Compute the mean and the variance of the training data for normalization.\n",
        "data_augmentation.layers[0].adapt(x_train)\n"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Me6HwOyFaYwW"
      },
      "source": [
        "## Implement multilayer perceptron (MLP)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z6mukUVmaYwW"
      },
      "source": [
        "\n",
        "def mlp(x, hidden_units, dropout_rate):\n",
        "    for units in hidden_units:\n",
        "        x = layers.Dense(units, activation=tf.nn.gelu)(x)\n",
        "        x = layers.Dropout(dropout_rate)(x)\n",
        "    return x\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SKNUm55DaYwW"
      },
      "source": [
        "## Implement patch creation as a layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KGUSdfNNaYwX"
      },
      "source": [
        "\n",
        "class Patches(keras.layers.Layer):\n",
        "    def __init__(self, patch_size):\n",
        "        super(Patches, self).__init__()\n",
        "        self.patch_size = patch_size\n",
        "\n",
        "    def call(self, images):\n",
        "        batch_size = tf.shape(images)[0]\n",
        "        patches = tf.image.extract_patches(\n",
        "            images=images,\n",
        "            sizes=[1, self.patch_size, self.patch_size, 1],\n",
        "            strides=[1, self.patch_size, self.patch_size, 1],\n",
        "            rates=[1, 1, 1, 1],\n",
        "            padding=\"VALID\",\n",
        "        )\n",
        "        patch_dims = patches.shape[-1]\n",
        "        patches = tf.reshape(patches, [batch_size, num_patches, patch_dims])\n",
        "        return patches\n"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dqA3m5C6aYwX"
      },
      "source": [
        "Let's display patches for a sample image"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iytQ3v-EaYwX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 547
        },
        "outputId": "4b1127c2-5815-4475-fa14-bcdb4c7c3dad"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(4, 4))\n",
        "image = x_train[np.random.choice(range(x_train.shape[0]))]\n",
        "plt.imshow(image.reshape([image_size, image_size]))\n",
        "plt.axis(\"off\")\n",
        "\n",
        "resized_image = tf.image.resize(\n",
        "    tf.convert_to_tensor([image]), size=(image_size, image_size)\n",
        ")\n",
        "patches = Patches(patch_size)(resized_image)\n",
        "print(f\"Image size: {image_size} X {image_size}\")\n",
        "print(f\"Patch size: {patch_size} X {patch_size}\")\n",
        "print(f\"Patches per image: {patches.shape[1]}\")\n",
        "print(f\"Elements per patch: {patches.shape[-1]}\")\n",
        "\n",
        "n = int(np.sqrt(patches.shape[1]))\n",
        "plt.figure(figsize=(4, 4))\n",
        "for i, patch in enumerate(patches[0]):\n",
        "    ax = plt.subplot(n, n, i + 1)\n",
        "    patch_img = tf.reshape(patch, (patch_size, patch_size, 1))\n",
        "    plt.imshow(patch_img.numpy().astype(\"uint8\").reshape([patch_size, patch_size]))\n",
        "    plt.axis(\"off\")"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image size: 28 X 28\n",
            "Patch size: 7 X 7\n",
            "Patches per image: 16\n",
            "Elements per patch: 49\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 288x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 288x288 with 16 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q3vrJRFsaYwX"
      },
      "source": [
        "## Implement the patch encoding layer\n",
        "\n",
        "The `PatchEncoder` layer will linearly transform a patch by projecting it into a\n",
        "vector of size `projection_dim`. In addition, it adds a learnable position\n",
        "embedding to the projected vector."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SlVB9myqaYwY"
      },
      "source": [
        "positional_embeddings = np.zeros((num_patches, projection_dim))\n",
        "for position in range(num_patches):\n",
        "    for i in range(0, projection_dim, 2):\n",
        "       positional_embeddings[position, i] = (\n",
        "                                          np.sin(position / (10000 ** ((2*i) / projection_dim)))\n",
        "                                            )\n",
        "       positional_embeddings[position, i + 1] = (\n",
        "                                              np.cos(position / (10000 ** ((2 * (i + 1) ) / projection_dim)))\n",
        "                                                )\n",
        "\n",
        "\n",
        "class PatchEncoder(keras.layers.Layer):\n",
        "    def __init__(self, num_patches, projection_dim):\n",
        "        super(PatchEncoder, self).__init__()\n",
        "        self.num_patches = num_patches\n",
        "        self.projection = layers.Dense(units=projection_dim)\n",
        "        self.position_embedding = tf.Variable(initial_value=positional_embeddings,\n",
        "                                              dtype=tf.float32, \n",
        "                                              trainable=False)\n",
        "        self.position_embedding = layers.Embedding(\n",
        "            input_dim=num_patches, output_dim=projection_dim\n",
        "        )\n",
        "        self.position_embedding = tf.Variable(initial_value=positional_embeddings,\n",
        "                                      dtype=tf.float32, \n",
        "                                      trainable=False)\n",
        "\n",
        "\n",
        "    def call(self, patch):\n",
        "        positions = tf.range(start=0, limit=self.num_patches, delta=1)\n",
        "        #encoded = self.projection(patch) + self.position_embedding(positions)\n",
        "        #encoded = self.projection(patch) + self.position_embedding\n",
        "        encoded = self.projection(patch)\n",
        "        return encoded\n"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Ct37nEzaYwY"
      },
      "source": [
        "## Build QMC model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jp57jQYuaYwY"
      },
      "source": [
        "\n",
        "def create_QMC_model():\n",
        "    inputs = keras.layers.Input(shape=input_shape)\n",
        "    # Augment data.\n",
        "    augmented = data_augmentation(inputs)\n",
        "    # Create patches.\n",
        "    patches = Patches(patch_size)(augmented)\n",
        "    # Encode patches.\n",
        "    # encoded_patches = PatchEncoder(num_patches, projection_dim)(patches)\n",
        "    fm_x1 = layers.QFeatureMapRFF(49, dim=num_rff , gamma=gamma)\n",
        "    psi_x = fm_x1(patches)\n",
        "    #ones = tf.ones_like(inputs[:, 0:1])\n",
        "    #rho_x = tf.keras.layers.concatenate((ones, psi_x), axis=1)\n",
        "    #rho_x = tf.expand_dims(rho_x, axis=-1)\n",
        "\n",
        "    # Create the Keras model.\n",
        "    model = keras.Model(inputs=inputs, outputs=psi_x)\n",
        "    return model\n"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 766
        },
        "id": "r8NtRtZWL0ym",
        "outputId": "b29f382d-d677-4d8b-e107-472f69bd50f8"
      },
      "source": [
        "model = create_QMC_model()\n",
        "model.summary()"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-96677b223763>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_QMC_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-23-a7d577720b38>\u001b[0m in \u001b[0;36mcreate_QMC_model\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m# encoded_patches = PatchEncoder(num_patches, projection_dim)(patches)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mfm_x1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mQFeatureMapRFF\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m49\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_rff\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgamma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mpsi_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfm_x1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpatches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0;31m#ones = tf.ones_like(inputs[:, 0:1])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;31m#rho_x = tf.keras.layers.concatenate((ones, psi_x), axis=1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    697\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    698\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ag_error_metadata'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 699\u001b[0;31m           \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    700\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m           \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Exception encountered when calling layer \"q_feature_map_rff_2\" (type QFeatureMapRFF).\n\nin user code:\n\n    File \"/usr/local/lib/python3.7/dist-packages/qmc/tf/layers.py\", line 181, in call  *\n        psi = vals / tf.expand_dims(norms, axis=-1)\n\n    ValueError: Dimensions must be equal, but are 16 and 512 for '{{node q_feature_map_rff_2/truediv}} = RealDiv[T=DT_FLOAT](q_feature_map_rff_2/mul, q_feature_map_rff_2/ExpandDims)' with input shapes: [?,16,512], [?,512,1].\n\n\nCall arguments received:\n  • inputs=tf.Tensor(shape=(None, 16, 49), dtype=float32)"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "raJsjdl3aYwY"
      },
      "source": [
        "## Compile, train, and evaluate the mode"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DrA5SfEhaYwY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9179efdb-27b8-42a9-d139-d469bd8efd1e"
      },
      "source": [
        "\n",
        "def run_experiment(model):\n",
        "    optimizer = tfa.optimizers.AdamW(\n",
        "        learning_rate=learning_rate, weight_decay=weight_decay\n",
        "    )\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=optimizer,\n",
        "        loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "        metrics=[\n",
        "            keras.metrics.SparseCategoricalAccuracy(name=\"accuracy\"),\n",
        "            keras.metrics.SparseTopKCategoricalAccuracy(5, name=\"top-5-accuracy\"),\n",
        "        ],\n",
        "    )\n",
        "\n",
        "    checkpoint_filepath = \"/tmp/checkpoint\"\n",
        "    checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
        "        checkpoint_filepath,\n",
        "        monitor=\"val_accuracy\",\n",
        "        save_best_only=True,\n",
        "        save_weights_only=True,\n",
        "    )\n",
        "\n",
        "    history = model.fit(\n",
        "        x=x_train,\n",
        "        y=y_train,\n",
        "        batch_size=batch_size,\n",
        "        epochs=num_epochs,\n",
        "        validation_split=0.1,\n",
        "        callbacks=[checkpoint_callback],\n",
        "    )\n",
        "\n",
        "    model.load_weights(checkpoint_filepath)\n",
        "    _, accuracy, top_5_accuracy = model.evaluate(x_test, y_test)\n",
        "    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n",
        "    print(f\"Test top 5 accuracy: {round(top_5_accuracy * 100, 2)}%\")\n",
        "\n",
        "    return history\n",
        "\n",
        "\n",
        "vit_classifier = create_vit_classifier()\n",
        "#vit_classifier.summary()\n",
        "history = run_experiment(vit_classifier)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "211/211 [==============================] - 6s 16ms/step - loss: 0.6631 - accuracy: 0.7945 - top-5-accuracy: 0.9726 - val_loss: 0.1528 - val_accuracy: 0.9568 - val_top-5-accuracy: 0.9977\n",
            "Epoch 2/10\n",
            "211/211 [==============================] - 3s 13ms/step - loss: 0.2950 - accuracy: 0.9095 - top-5-accuracy: 0.9954 - val_loss: 0.1007 - val_accuracy: 0.9692 - val_top-5-accuracy: 0.9988\n",
            "Epoch 3/10\n",
            "211/211 [==============================] - 3s 14ms/step - loss: 0.2246 - accuracy: 0.9306 - top-5-accuracy: 0.9974 - val_loss: 0.0861 - val_accuracy: 0.9757 - val_top-5-accuracy: 0.9982\n",
            "Epoch 4/10\n",
            "211/211 [==============================] - 3s 15ms/step - loss: 0.1839 - accuracy: 0.9431 - top-5-accuracy: 0.9985 - val_loss: 0.0794 - val_accuracy: 0.9777 - val_top-5-accuracy: 0.9987\n",
            "Epoch 5/10\n",
            "211/211 [==============================] - 3s 15ms/step - loss: 0.1612 - accuracy: 0.9493 - top-5-accuracy: 0.9986 - val_loss: 0.0671 - val_accuracy: 0.9795 - val_top-5-accuracy: 0.9995\n",
            "Epoch 6/10\n",
            "211/211 [==============================] - 3s 13ms/step - loss: 0.1479 - accuracy: 0.9544 - top-5-accuracy: 0.9989 - val_loss: 0.0609 - val_accuracy: 0.9825 - val_top-5-accuracy: 0.9990\n",
            "Epoch 7/10\n",
            "211/211 [==============================] - 3s 13ms/step - loss: 0.1338 - accuracy: 0.9595 - top-5-accuracy: 0.9990 - val_loss: 0.0621 - val_accuracy: 0.9822 - val_top-5-accuracy: 0.9993\n",
            "Epoch 8/10\n",
            "211/211 [==============================] - 3s 14ms/step - loss: 0.1253 - accuracy: 0.9608 - top-5-accuracy: 0.9991 - val_loss: 0.0573 - val_accuracy: 0.9828 - val_top-5-accuracy: 0.9990\n",
            "Epoch 9/10\n",
            "211/211 [==============================] - 3s 13ms/step - loss: 0.1159 - accuracy: 0.9646 - top-5-accuracy: 0.9992 - val_loss: 0.0542 - val_accuracy: 0.9833 - val_top-5-accuracy: 0.9993\n",
            "Epoch 10/10\n",
            "211/211 [==============================] - 3s 14ms/step - loss: 0.1100 - accuracy: 0.9647 - top-5-accuracy: 0.9994 - val_loss: 0.0540 - val_accuracy: 0.9847 - val_top-5-accuracy: 0.9992\n",
            "313/313 [==============================] - 2s 6ms/step - loss: 0.0622 - accuracy: 0.9808 - top-5-accuracy: 0.9999\n",
            "Test accuracy: 98.08%\n",
            "Test top 5 accuracy: 99.99%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WAB4UekdrMO6",
        "outputId": "91e9e189-a09e-423d-eb62-6bf93a13d91c"
      },
      "source": [
        "vit_classifier.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_23\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_32 (InputLayer)          [(None, 28, 28, 1)]  0           []                               \n",
            "                                                                                                  \n",
            " data_augmentation (Sequential)  (None, 28, 28, 1)   3           ['input_32[0][0]']               \n",
            "                                                                                                  \n",
            " patches_41 (Patches)           (None, 16, 49)       0           ['data_augmentation[24][0]']     \n",
            "                                                                                                  \n",
            " patch_encoder_31 (PatchEncoder  (None, 16, 64)      4224        ['patches_41[0][0]']             \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " layer_normalization_106 (Layer  (None, 16, 64)      128         ['patch_encoder_31[0][0]']       \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " multi_head_attention_39 (Multi  (None, 16, 64)      66368       ['layer_normalization_106[0][0]',\n",
            " HeadAttention)                                                   'layer_normalization_106[0][0]']\n",
            "                                                                                                  \n",
            " add_78 (Add)                   (None, 16, 64)       0           ['multi_head_attention_39[0][0]',\n",
            "                                                                  'patch_encoder_31[0][0]']       \n",
            "                                                                                                  \n",
            " layer_normalization_107 (Layer  (None, 16, 64)      128         ['add_78[0][0]']                 \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " dense_161 (Dense)              (None, 16, 128)      8320        ['layer_normalization_107[0][0]']\n",
            "                                                                                                  \n",
            " dropout_129 (Dropout)          (None, 16, 128)      0           ['dense_161[0][0]']              \n",
            "                                                                                                  \n",
            " dense_162 (Dense)              (None, 16, 64)       8256        ['dropout_129[0][0]']            \n",
            "                                                                                                  \n",
            " dropout_130 (Dropout)          (None, 16, 64)       0           ['dense_162[0][0]']              \n",
            "                                                                                                  \n",
            " add_79 (Add)                   (None, 16, 64)       0           ['dropout_130[0][0]',            \n",
            "                                                                  'add_78[0][0]']                 \n",
            "                                                                                                  \n",
            " layer_normalization_108 (Layer  (None, 16, 64)      128         ['add_79[0][0]']                 \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " flatten_26 (Flatten)           (None, 1024)         0           ['layer_normalization_108[0][0]']\n",
            "                                                                                                  \n",
            " dropout_131 (Dropout)          (None, 1024)         0           ['flatten_26[0][0]']             \n",
            "                                                                                                  \n",
            " dense_163 (Dense)              (None, 256)          262400      ['dropout_131[0][0]']            \n",
            "                                                                                                  \n",
            " dropout_132 (Dropout)          (None, 256)          0           ['dense_163[0][0]']              \n",
            "                                                                                                  \n",
            " dense_164 (Dense)              (None, 10)           2570        ['dropout_132[0][0]']            \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 352,525\n",
            "Trainable params: 351,498\n",
            "Non-trainable params: 1,027\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YCXm-EtMaYwZ"
      },
      "source": [
        "After 100 epochs, the ViT model achieves around 55% accuracy and\n",
        "82% top-5 accuracy on the test data. These are not competitive results on the CIFAR-100 dataset,\n",
        "as a ResNet50V2 trained from scratch on the same data can achieve 67% accuracy.\n",
        "\n",
        "Note that the state of the art results reported in the\n",
        "[paper](https://arxiv.org/abs/2010.11929) are achieved by pre-training the ViT model using\n",
        "the JFT-300M dataset, then fine-tuning it on the target dataset. To improve the model quality\n",
        "without pre-training, you can try to train the model for more epochs, use a larger number of\n",
        "Transformer layers, resize the input images, change the patch size, or increase the projection dimensions. \n",
        "Besides, as mentioned in the paper, the quality of the model is affected not only by architecture choices, \n",
        "but also by parameters such as the learning rate schedule, optimizer, weight decay, etc.\n",
        "In practice, it's recommended to fine-tune a ViT model\n",
        "that was pre-trained using a large, high-resolution dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J3tN7EYJb3z4"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}